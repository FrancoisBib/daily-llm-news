# Veille IA & LLM – 24 décembre 2025

> Bulletin quotidien automatisé – sélection des 5 actualités marquantes dans le domaine de l’intelligence artificière et des grands modèles de langage.

---

## 1. Les modèles open-source chinois séduisent les développeurs occidentaux malgré les tensions géopolitiques
**Source :** [The Register](https://www.theregister.com/2025/12/23/chinese_open_source_ai_models/) *(23 déc. 2025)*  

Face aux restrictions commerciales croissantes, les LLM open-source chinois – notamment Qwen-72B et ChatGLM3 – connaissent une adoption rapide sur GitHub et Hugging Face. Leur performance proche de GPT-4 à moindre coût attire start-ups et scale-ups européennes qui cherchent à réduire leur dépendance aux API étasuniennes. Le phénomène accélère la balkanisation technologique et pousse la Commission européenne à finaliser son « AI Cloud Act » pour encadrer les flux de données d’entraînement.

---

## 2. OpenAI lance « ChatGPT-5-mini », une version allégée multimodale et 40 % moins chère
**Source :** [TechCrunch](https://techcrunch.com/2025/12/24/openai-chatgpt-5-mini-release/) *(24 déc. 2025)*  

Disponible dès aujourd’hui via l’API et l’interface web, ChatGPT-5-mini accepte texte, image et son dans un seul contexte de 256 k tokens. Le modèle vise les développeurs d’apps mobiles et les PME européennes soumises à la tarification dynamique. Première concession notable : OpenAI publie pour la première fois une fiche technique partielle détaillant le pipeline de filtrage des données d’entraînement, afin de répondre aux critiques sur le respect du copyright et de la vie privée.

---

## 3. Google intègre Gemini 2.0 Pro dans Android Studio, permettant la génération locale de code offline
**Source :** [VentureBeat](https://venturebeat.com/ai/google-gemini-2-pro-android-studio-offline/) *(24 déc. 2025)*  

Android Studio Hedgehog 2025.3.1 embarque une version quantifiée de Gemini 2.0 Pro qui tourne entièrement sur GPU consommateurs (24 Go VRAM). L’écosystème gradle profite d’un plugin de « completion sémantique » : le modèle anticipe les intentions du développeur et génère des blocs de code Kotlin/Java exploitables sans connexion. Côté sécurité, Google assure que le modèle reste sur la machine, réduisant les risques de fuite de code source propriétaire.

---

## 4. Le Parlement européen valide le « AI Liability Directive » : responsabilité sans faute pour les fournisseurs de LLM
**Source :** [Euractiv](https://www.euractiv.com/section/artificial-intelligence/news/ai-liability-directive-approved-2025/) *(23 déc. 2025)*  

Les députés ont adopté en lecture plénière le texte qui impose aux éditeurs de modèles généralistes une présomption de responsabilité en cas de dommage causé par leurs systèmes (désinformation, discrimination, atteinte à la vie privée). Les entreprises devront prouver qu’elles ont mis en œuvre « toutes mesures de sécurité disponibles », incluant l’audit tierce partie et la traçabilité des données d’entraînement. Le vote ouvre la voie à des recours collectifs transfrontaliers dès 2026.

---

## 5. Une faille « zero-click » dans une librairie d’inférence LLM populaire expose 12 M de serveurs
**Source :** [Ars Technica](https://arstechnica.com/security/2025/12/zero-click-vulnerability-llm-inference-library/) *(24 déc. 2025)*  

La bibliothèque « FastLLM Core » (utilisée par des cadres de déploiement de Modèles de Langage) comporte une vulnérabilité de désérialisation dans son parser de graphes de calcul. Un paquet malveillant peut être expédié via une requête d’inférence et exécuter du code arbitraire sur le serveur hôte sans authentification. Les mainteneurs ont publié le patch 1.9.4 ; le CVE-2025-7421 est coté 9.8/10. Hetzner, AWS et Azure recommandent la mise à jour immédiate des images conteneurisées.

---

*Fin du bulletin du 24 décembre 2025 – génération automatisée.*